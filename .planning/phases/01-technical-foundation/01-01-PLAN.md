---
phase: 01-technical-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/robots.ts
  - app/sitemap.ts
autonomous: true

must_haves:
  truths:
    - "robots.txt explicitly allows GPTBot, OAI-SearchBot, ChatGPT-User, ClaudeBot, claude-web, PerplexityBot, Perplexity-User, Google-Extended, and Googlebot"
    - "sitemap.xml is generated with all pages (7 static + 15 articles = 22 entries)"
    - "sitemap.xml includes hreflang alternates for en/ja on every entry"
    - "sitemap.xml is referenced in robots.txt"
  artifacts:
    - path: "app/robots.ts"
      provides: "AI crawler allow rules and sitemap reference"
      min_lines: 25
      exports: ["default"]
    - path: "app/sitemap.ts"
      provides: "Bilingual sitemap with hreflang alternates"
      min_lines: 40
      exports: ["default"]
  key_links:
    - from: "app/robots.ts"
      to: "/sitemap.xml"
      via: "sitemap property in return object"
      pattern: "sitemap.*sitemap\\.xml"
    - from: "app/sitemap.ts"
      to: "lib/content/index.ts"
      via: "getAllArticles import"
      pattern: "import.*getAllArticles.*from.*lib/content"
    - from: "app/sitemap.ts"
      to: "routing.ts"
      via: "routing.locales for locale list"
      pattern: "import.*routing.*from.*routing"
---

<objective>
Implement robots.txt and sitemap.xml for AI crawler access

Purpose: Enable AI search crawlers (GPTBot, ClaudeBot, PerplexityBot, Googlebot) to discover, access, and index all site content with proper bilingual hreflang signals.

Output:
- `/robots.txt` at site root allowing AI crawlers
- `/sitemap.xml` at site root with 22 bilingual entries (7 static pages + 15 articles, each with en/ja alternates)
</objective>

<execution_context>
@/Users/kazuya/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kazuya/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-technical-foundation/01-RESEARCH.md
@routing.ts
@lib/content/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create robots.ts with AI crawler allow rules</name>
  <files>app/robots.ts</files>
  <action>
Create `app/robots.ts` (at app root, NOT inside [locale]) that exports a default function returning `MetadataRoute.Robots`.

Include explicit allow rules for all AI search crawlers:
- OpenAI: GPTBot, OAI-SearchBot, ChatGPT-User
- Anthropic: ClaudeBot, claude-web
- Perplexity: PerplexityBot, Perplexity-User
- Google: Googlebot, Google-Extended

Default rule for all other crawlers:
- Allow: /
- Disallow: /api/, /_next/

Include sitemap reference using baseUrl from NEXT_PUBLIC_SITE_URL env var with fallback to 'https://kazuya.work'.

Pattern from research:
```typescript
import type { MetadataRoute } from 'next'

export default function robots(): MetadataRoute.Robots {
  const baseUrl = process.env.NEXT_PUBLIC_SITE_URL || 'https://kazuya.work'
  return {
    rules: [
      { userAgent: 'GPTBot', allow: '/' },
      // ... all AI crawlers
      { userAgent: '*', allow: '/', disallow: ['/api/', '/_next/'] },
    ],
    sitemap: `${baseUrl}/sitemap.xml`,
  }
}
```
  </action>
  <verify>
1. `npx tsc --noEmit` passes (no TypeScript errors)
2. File exists at app/robots.ts (not app/[locale]/robots.ts)
3. Run `npm run build` and check .next/server/app/robots.txt.body exists
  </verify>
  <done>
robots.ts compiles without errors, is at correct location (app root), and contains all 9 AI crawler allow rules plus sitemap reference.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create sitemap.ts with bilingual hreflang alternates</name>
  <files>app/sitemap.ts</files>
  <action>
Create `app/sitemap.ts` (at app root, NOT inside [locale]) that exports a default function returning `MetadataRoute.Sitemap`.

Static pages to include (7 total):
- '' (homepage) - priority: 1.0, changeFrequency: 'weekly'
- 'about' - priority: 0.8, changeFrequency: 'monthly'
- 'what-i-do' - priority: 0.9, changeFrequency: 'monthly'
- 'process' - priority: 0.7, changeFrequency: 'monthly'
- 'cases' - priority: 0.8, changeFrequency: 'weekly'
- 'contact' - priority: 0.6, changeFrequency: 'yearly'
- 'privacy' - priority: 0.3, changeFrequency: 'yearly'

Dynamic articles: Import and use `getAllArticles()` from `@/lib/content` to get all 15 articles (12 workflows + 3 case studies). Articles are at `/cases/[slug]` path with priority: 0.7, changeFrequency: 'monthly'.

Every entry MUST include:
- url: Full URL with /en/ prefix (canonical)
- lastModified: new Date()
- changeFrequency: as specified above
- priority: as specified above
- alternates.languages: Object with both 'en' and 'ja' URLs

Use baseUrl from NEXT_PUBLIC_SITE_URL env var with fallback to 'https://kazuya.work'.

Pattern from research:
```typescript
import type { MetadataRoute } from 'next'
import { getAllArticles } from '@/lib/content'

export default function sitemap(): MetadataRoute.Sitemap {
  const baseUrl = process.env.NEXT_PUBLIC_SITE_URL || 'https://kazuya.work'
  const entries: MetadataRoute.Sitemap = []

  // Static pages
  const staticPages = [
    { path: '', priority: 1.0, changeFreq: 'weekly' as const },
    // ... other pages
  ]

  for (const page of staticPages) {
    const path = page.path ? `/${page.path}` : ''
    entries.push({
      url: `${baseUrl}/en${path}`,
      lastModified: new Date(),
      changeFrequency: page.changeFreq,
      priority: page.priority,
      alternates: {
        languages: {
          en: `${baseUrl}/en${path}`,
          ja: `${baseUrl}/ja${path}`,
        },
      },
    })
  }

  // Article pages
  const articles = getAllArticles()
  for (const article of articles) {
    entries.push({
      url: `${baseUrl}/en/cases/${article.slug}`,
      // ... with alternates
    })
  }

  return entries
}
```
  </action>
  <verify>
1. `npx tsc --noEmit` passes (no TypeScript errors)
2. File exists at app/sitemap.ts (not app/[locale]/sitemap.ts)
3. Run `npm run build` and check .next/server/app/sitemap.xml.body exists
4. Count entries: should be 22 (7 static + 15 articles)
  </verify>
  <done>
sitemap.ts compiles without errors, is at correct location (app root), generates 22 entries with proper hreflang alternates for both en and ja locales.
  </done>
</task>

</tasks>

<verification>
After completing both tasks:

1. **Build verification:**
   ```bash
   npm run build
   ```
   Build should complete without errors.

2. **File location check:**
   ```bash
   ls -la app/robots.ts app/sitemap.ts
   ```
   Both files should exist at app root (NOT inside [locale]).

3. **Output file check:**
   ```bash
   ls .next/server/app/robots.txt.body .next/server/app/sitemap.xml.body
   ```
   Both output files should exist after build.

4. **robots.txt content check:**
   ```bash
   cat .next/server/app/robots.txt.body
   ```
   Should contain:
   - User-agent: GPTBot with Allow: /
   - User-agent: ClaudeBot with Allow: /
   - User-agent: PerplexityBot with Allow: /
   - Sitemap: https://kazuya.work/sitemap.xml

5. **sitemap.xml entry count:**
   ```bash
   grep -c '<url>' .next/server/app/sitemap.xml.body
   ```
   Should return 22 (7 static pages + 15 articles).

6. **hreflang verification:**
   ```bash
   grep -c 'hreflang="ja"' .next/server/app/sitemap.xml.body
   ```
   Should return 22 (one ja alternate for each entry).
</verification>

<success_criteria>
1. robots.txt allows all 9 specified AI crawlers (GPTBot, OAI-SearchBot, ChatGPT-User, ClaudeBot, claude-web, PerplexityBot, Perplexity-User, Google-Extended, Googlebot)
2. robots.txt includes Sitemap directive pointing to /sitemap.xml
3. sitemap.xml contains exactly 22 URL entries (7 static + 15 articles)
4. Every sitemap entry has both en and ja hreflang alternates
5. Both files are generated at site root (not locale-prefixed)
6. npm run build completes without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-technical-foundation/01-01-SUMMARY.md`
</output>
